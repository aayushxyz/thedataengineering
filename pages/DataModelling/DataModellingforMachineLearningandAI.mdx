---
title: Data Modelling for Machine Learning and AI
description: Adapting data modelling techniques to support machine learning and AI applications
---

# Data Modelling for Machine Learning and AI

## Introduction

As the field of data engineering continues to evolve, the requirements for data modelling have also shifted to accommodate the growing needs of machine learning (ML) and artificial intelligence (AI) applications. Traditional data modelling techniques, which were primarily focused on optimizing data storage and querying, are now being adapted to support the unique demands of ML and AI use cases. In this article, we will explore the key considerations and best practices for data modelling in the context of ML and AI projects.

## Data Modelling for ML/AI: Unique Requirements

The primary focus of ML and AI applications is often on feature engineering and model training, rather than the traditional concerns of data storage and querying. This shift in priorities necessitates a different approach to data modelling, with the following key considerations:

1. **Feature-Centric Design**: In ML and AI, the emphasis is on extracting relevant features from the data to train predictive models. The data model must be designed to facilitate the feature engineering process, ensuring that the necessary data attributes are easily accessible and can be efficiently transformed into model-ready features.

2. **Flexibility and Iterative Development**: ML and AI projects often involve an iterative process of experimentation, where the data model needs to be flexible enough to accommodate changes in feature requirements, model architectures, and data sources. The data model should be designed with this agility in mind, allowing for easy modifications and extensions.

3. **Handling Diverse Data Types**: ML and AI applications often work with a wide range of data types, including structured, semi-structured, and unstructured data. The data model must be capable of handling this diversity, ensuring that all relevant data can be seamlessly integrated and processed.

4. **Data Lineage and Provenance**: Tracking the origin, transformation, and usage of data is crucial in ML and AI projects, as it enables model interpretability, debugging, and compliance. The data model should incorporate mechanisms for capturing and maintaining data lineage and provenance information.

5. **Scalability and Performance**: As ML and AI models become more complex and the volume of data grows, the underlying data model must be able to scale efficiently to support the increased computational and storage requirements.

## Data Modelling Approaches for ML/AI

To address the unique requirements of ML and AI applications, data engineers can leverage the following data modelling approaches:

1. **Feature-Oriented Data Modelling**: In this approach, the data model is designed around the features required by the ML/AI models, rather than the traditional entity-relationship model. This involves identifying the key features, their relationships, and the necessary transformations to prepare the data for model training.

   Example: For a customer churn prediction model, the data model might include entities such as "Customer," "Transaction," and "Interaction," with features like "average_transaction_amount," "number_of_complaints," and "days_since_last_purchase."

2. **Data Mesh Architecture**: The data mesh approach promotes a decentralized, domain-driven data architecture, where each domain owns and manages its own data assets. This aligns well with the feature-centric nature of ML/AI, as each domain can optimize its data model for the specific needs of its ML/AI use cases.

   ```mermaid
   graph LR
   A[Domain 1] --> B[Domain 2]
   B --> C[Domain 3]
   C --> D[Domain 4]
   D --> E[Data Catalog]
   ```

3. **Data Catalog and Metadata Management**: Effective metadata management is crucial in ML/AI data modelling, as it enables the discovery, understanding, and governance of data assets. A well-designed data catalog can serve as the central hub for managing metadata, including data lineage, data quality, and feature definitions.

   ```mermaid
   graph LR
   A[Data Sources] --> B[Data Ingestion]
   B --> C[Data Transformation]
   C --> D[Data Storage]
   D --> E[Data Catalog]
   E --> F[ML/AI Models]
   ```

4. **Data Virtualization and Federated Data Models**: In complex ML/AI scenarios, where data is distributed across multiple sources, data virtualization and federated data models can provide a unified view of the data, enabling seamless feature extraction and model training.

   ```mermaid
   graph LR
   A[Data Source 1] --> B[Data Virtualization Layer]
   C[Data Source 2] --> B
   D[Data Source 3] --> B
   B --> E[ML/AI Models]
   ```

5. **Event-Driven Data Modelling**: For real-time or streaming ML/AI applications, an event-driven data modelling approach can be beneficial. This involves designing the data model around the events that trigger feature updates or model retraining, ensuring that the data is readily available for immediate processing.

   ```mermaid
   graph LR
   A[Event Source 1] --> B[Event Streaming Platform]
   C[Event Source 2] --> B
   B --> D[Feature Engineering]
   D --> E[ML/AI Models]
   ```

## Data Modelling Patterns for ML/AI

To support the unique requirements of ML and AI applications, data engineers can leverage the following data modelling patterns:

1. **Data Lake House**: This pattern combines the flexibility and scalability of a data lake with the structured data management capabilities of a data warehouse, providing a unified platform for both analytical and ML/AI workloads.

2. **Feature Store**: A feature store is a centralized repository for storing and managing the features used in ML/AI models. It provides a consistent and reliable way to access and reuse features across multiple models and projects.

3. **Hybrid Transactional/Analytical Processing (HTAP)**: HTAP data models support both transactional and analytical workloads, enabling real-time feature extraction and model training without the need for complex data pipelines.

4. **Temporal Data Modelling**: This pattern captures the evolution of data over time, which is crucial for understanding the dynamic nature of features and their impact on ML/AI models.

5. **Polyglot Persistence**: By leveraging a combination of data storage technologies (e.g., relational databases, NoSQL databases, object stores), polyglot persistence data models can accommodate the diverse data requirements of ML/AI applications.

## Conclusion

Data modelling for ML and AI applications requires a shift in focus from traditional data storage and querying concerns to feature engineering and model training requirements. By adopting feature-centric design, leveraging data mesh architectures, implementing robust metadata management, and utilizing data modelling patterns tailored for ML/AI, data engineers can build data platforms that effectively support the unique needs of these cutting-edge technologies. As the field of data engineering continues to evolve, mastering these data modelling techniques will be crucial for delivering successful ML and AI solutions.